{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# OLEKSANDR LYTVYN\n",
    "### Projekt 1\n",
    "#### git: https://github.com/letv3/ADT2021"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ollyt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sqlalchemy import create_engine"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Connect to local db"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engine(postgresql://postgres:***@localhost:5432/tweets_pdt21)\n"
     ]
    }
   ],
   "source": [
    "pg_user = 'postgres'\n",
    "pg_password = 'postgres'\n",
    "pg_address = 'localhost'\n",
    "pg_database = 'tweets_pdt21'\n",
    "# connection = psycopg2.connect(dbname=pg_database,\n",
    "#                               user=pg_user,\n",
    "#                               password=pg_password,\n",
    "#                               host=pg_address)\n",
    "connection = create_engine(f'postgresql://{pg_user}:{pg_password}'\n",
    "                           f'@{pg_address}:5432/{pg_database}')\n",
    "print(connection)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "FILTER ALL CONSPIRACY TWEETS (uloha 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "conspiracy_hashtags_ids_df = pd.read_sql_query(\"\"\"\n",
    "    SELECT h.id FROM hashtags h WHERE lower(h.value) IN\n",
    "    (SELECT lower(hashtag_value) FROM conspiracy_hashtags)\n",
    "\"\"\", connection)\n",
    "conspiracy_hashtags_ids_string = ','.join(conspiracy_hashtags_ids_df['id'].astype(str))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "conspiracy_tweets_df = pd.read_sql_query(f\"\"\"\n",
    "    SELECT tweets.id, tweets.content FROM tweets WHERE tweets.id IN (\n",
    "        SELECT th.tweet_id FROM tweet_hashtags th WHERE th.hashtag_id IN (\n",
    "            SELECT h.id FROM hashtags h WHERE lower(h.value) IN (\n",
    "                SELECT lower(hashtag_value) FROM conspiracy_hashtags\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\"\"\", connection)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create table for the conspiracy tweets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "conspiracy_tweets_df.to_sql(name='conspiracy_tweets',\n",
    "                            con=connection,\n",
    "                            index=False,\n",
    "                            chunksize=10000)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from src.project_one import TweetProcessor\n",
    "\n",
    "batch_size = 10000\n",
    "n_batches = int(conspiracy_tweets_df['id'].count()/batch_size) + 1\n",
    "processed_tweets = []\n",
    "\n",
    "tp = TweetProcessor()\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_tweet(tweet_content, tweet_id) -> dict:\n",
    "    \"\"\"Sentiment analysis for tweet text and\n",
    "    and constructing dataframe row entry\"\"\"\n",
    "    prepared_tweet = tp.prepare_tweet(tweet_content)\n",
    "    tweet_sentiment = sid.polarity_scores(prepared_tweet)\n",
    "    tweet_sentiment.update({'id': tweet_id})\n",
    "    return tweet_sentiment\n",
    "\n",
    "def process_batch_of_tweets(tweets, batch_size: int):\n",
    "    processed_batch = []\n",
    "    for i in range(batch_size):\n",
    "        analyzed_tweet = analyze_tweet(tweets['content'].iloc[i], tweets['id'].iloc[i])\n",
    "        processed_batch.append(analyzed_tweet)\n",
    "    return processed_batch\n",
    "\n",
    "batched_tweets = []\n",
    "for i in range(1,215):\n",
    "    batched_tweets.append(conspiracy_tweets_df.iloc[batch_size*(i-1):batch_size*i])\n",
    "    processed_tweets.extend(process_batch_of_tweets(batched_tweets, batch_size))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Multiprocessing tweet batches some shit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "with mp.Pool(processes=4) as pool:\n",
    "    processed_tweets = pool.map(process_batch_of_tweets, [batched_tweets[0:4],\n",
    "                                                          batched_tweets[4:8],\n",
    "                                                          batched_tweets[8:12],\n",
    "                                                          batched_tweets[12:16],\n",
    "                                                          batched_tweets[16:19]])\n",
    "\n",
    "processed_tweets\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "19"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conspiracy_tweets_df.count()\n",
    "len(batched_tweets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}